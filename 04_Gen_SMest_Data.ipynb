{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# IMPORT LIBRARIES\n",
    "# ---------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "data_path = '/path/hru_region02_np_arr'\n",
    "save_dir = '/path/HRU_TGCN_sub/'\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# LOAD FEATURE NAMES AND DESCRIPTIONS\n",
    "# Feature description is a dictionary object that \n",
    "# contains feature_name: feature definition pairs \n",
    "# based on SWAT IO documentation\n",
    "# --------------------------------------------\n",
    "with open('/path/SWAT_feat_names.pkl', 'rb') as f:\n",
    "    feat_names = pickle.load(f)\n",
    "with open('/path/SWAT_feat_names.pkl', 'rb') as f:\n",
    "    feat_names = pickle.load(f)\n",
    "\n",
    "    \n",
    "# ---------------------------------------------\n",
    "# SUBSET OF FEATURES FOR SM PREDICTION\n",
    "# **************** USER INPUT *****************\n",
    "# Define features used for estimating soil moisture\n",
    "# --------------------------------------------- \n",
    "sub_feat_names = ['AREA', 'PRECIP' , 'ET', 'SW_END', 'PERC', 'GW_RCHG', 'DA_RCHG', 'REVAP', 'SA_IRR', 'DA_IRR', 'SA_ST', 'DA_ST',\n",
    "                 'WYLD', 'DAILYCN', 'TMP_AV', 'SOL_TMP', 'SOLAR']\n",
    "\n",
    "watershed_names_list = os.listdir(data_path)\n",
    "if '.DS_Store' in watershed_names_list: names_list.remove('.DS_Store')\n",
    "if 'README' in watershed_names_list: names_list.remove('README')\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------\n",
    "# INDEPENDENT HRU FILES\n",
    "# Instead of HRUs clumped together in watersheds\n",
    "# associate universal IDs with the hrus for tracability\n",
    "# and save as independent files.\n",
    "# --------------------------------------------- \n",
    "clustering_feature_names = feat_names[5:]\n",
    "n_feat = len(sub_feat_names) \n",
    "n_tstep = 38*12\n",
    "\n",
    "for name in watershed_names_list:\n",
    "    features = np.load(data_path+'/'+name)\n",
    "\n",
    "    # Delete annual summary (over a single year)\n",
    "    # Delete simulation summary  (over 38 years)    \n",
    "    df = pd.DataFrame(features)\n",
    "    df.columns= feat_names[5:86]    \n",
    "    df.drop(df[df.MON > 12].index, inplace=True)\n",
    "    n_hru = df.MON.ne(1).idxmax()\n",
    "\n",
    "    # select a subset of features \n",
    "    sub_df = df[sub_feat_names]\n",
    "\n",
    "    # Rearrange data so data is in the format ( monthly time step, hrus, features) \n",
    "    num_features = sub_df.to_numpy()\n",
    "    num_features = num_features.reshape(n_tstep, n_hru, n_feat )  \n",
    "\n",
    "    # Save the data for each hru (time step , features)\n",
    "    for i in range(n_hru):                                 \n",
    "        np.save(save_dir+name.split('_')[0]+'.'+str(n_hru)+'.'+str(i), num_features[:,i,:])\n",
    "        \n",
    "# load the saved HRU names that correspond to the order \n",
    "# of hrus saved clusterwise in the cluster specific distance array.  \n",
    "with open('/path/hru_names_clusterwise.pkl', 'rb') as f:  hru_names_clusterwise = pickle.load(f)      \n",
    "\n",
    "    \n",
    "# ---------------------------------------------\n",
    "# Load hrus and save as anumpy array each array \n",
    "# is ordered according to the results from 03_clustering. \n",
    "# This along with the cluster-specific distance array \n",
    "# is used to generate the graphs. \n",
    "# --------------------------------------------- \n",
    "for i in range(12):\n",
    "    n_hrus = len(hru_names_clusterwise[i])\n",
    "\n",
    "    # Read hru data  \n",
    "    # !!! Make sure data is in format (timeseries, hrus , features)\n",
    "    data = np.zeros((n_tstep, n_hrus , n_feat))\n",
    "    for j, name in enumerate(hru_names_clusterwise[i]):\n",
    "        data[:,j,:] = np.load('/path/'+name+'.npy') # (tstep , features)\n",
    "        \n",
    "    np.save('/path/sm_est_hru_data_clstr_'+str(i),data)\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------\n",
    "# VISUALIZE GRAPH FROM CLUSTER \n",
    "# Run at your own risk - Very slow for large graphs\n",
    "# you have been warned !! \n",
    "# --------------------------------------------- \n",
    "\n",
    "# import networkx as nx\n",
    "# from torch_geometric.utils.convert import to_networkx\n",
    "# from sklearn.neighbors import radius_neighbors_graph\n",
    "\n",
    "# cluster_id = 6\n",
    "# clstr_distance = np.load('/path/'+'dist.in.cluster_'+str(cluster_id)+'.npy')\n",
    "# adj_csr = radius_neighbors_graph( clstr_distance, 1, include_self = True)\n",
    "\n",
    "# G = nx.Graph(adj_csr)\n",
    "# nx.draw(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
