{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# IMPORT LIBRARIES\n",
    "# ---------------------------------------------\n",
    "import numpy as np\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax , TimeSeriesScalerMeanVariance\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# DEPENDENCIES \n",
    "# the following depend on 01_Gen_Clustering_Data:\n",
    "# - clustering_feature_names\n",
    "# - n_hrus\n",
    "# - all_data\n",
    "# ---------------------------------------------\n",
    "with open('/Users/Documents/GitHub/GNNs_PrecisionAgriculture/SWAT Data Reader/clustering_feature_names.pkl', 'rb') as f:\n",
    "    clustering_feature_names = pickle.load(f)\n",
    "n_feat = len(feat_names)\n",
    "\n",
    "data_path = '/Users/muneeza/Documents/GitHub/DATA_SMest/HRU_clustering'\n",
    "save_path = '/Users/muneeza/Documents/GitHub/DATA_SMest/HRU_Clustering_results/'\n",
    "names_list = os.listdir(data_path)\n",
    "names_list.remove('.DS_Store')\n",
    "\n",
    "# Since every watershed file has different number of HRUs \n",
    "# we strip the number of HRUs from the file name and \n",
    "# aggregate to allocate memory for all HRUs in region02\n",
    "n_hrus = np.array([x.split('.')[-2] for x in names_list]).astype(int)\n",
    "hrus_total = np.sum(n_hrus)\n",
    "all_data= np.zeros((12, hrus_total,10))   # (months, hrus, features)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# LOAD DATA\n",
    "# loop through all watershed data and concatenate \n",
    "# all hrus in one array\n",
    "# ---------------------------------------------\n",
    "st = 0\n",
    "en = 0\n",
    "for i, name in enumerate(names_list):\n",
    "    en += n_hrus[i]\n",
    "    all_data[:,st:en,:] = np.load(data_path+'/'+name)\n",
    "    st = en\n",
    "\n",
    "all_data = all_data.transpose(1,0,2)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# DEFINE NORMALIZATION AND CLUSTERING FUNCTIONS\n",
    "# ---------------------------------------------\n",
    "def normalization(type, all_data,n_feat):\n",
    "    if type =='custom':\n",
    "        max_arr = np.zeros(n_feat)\n",
    "        min_arr = np.zeros(n_feat)\n",
    "        X_train_norm = np.zeros(all_data.shape)\n",
    "        for i in range(n_feat):\n",
    "            max_arr[i] = np.max(all_data[:,:,i])\n",
    "            min_arr[i] = np.min(all_data[:,:,i])\n",
    "            X_train_norm[:,:,i] = (all_data[:,:,i] -  min_arr[i])/( max_arr[i]- min_arr[i])\n",
    "    elif type == 'minmax':\n",
    "        X_train_norm = TimeSeriesScalerMinMax(value_range=(0,1)).fit_transform(all_data)\n",
    "    elif type == 'std':\n",
    "        X_train_norm = TimeSeriesScalerMeanVariance(0,1).fit_transform(all_data)\n",
    "    else: \n",
    "        X_train_norm = all_data\n",
    "    return(X_train_norm)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# NORMALIZE DATA AND PERFORM CLUSTERING\n",
    "# Data (n_ts , sz, d) \n",
    "# n_ts : number of time series (hrus)\n",
    "# sz : size of time series (n time steps)\n",
    "# d : dimension of data (n features)\n",
    "# CAUTION !! data shape must be (hrus , timesteps , features )\n",
    "# ---------------------------------------------\n",
    "X_train_norm = normalization('custom', all_data, n_feat)\n",
    "\n",
    "model = TimeSeriesKMeans(n_clusters=12, metric=\"dtw\", max_iter=10)\n",
    "model.fit(X_train_norm)\n",
    "labels = model.labels_\n",
    "dist = model.transform(X_train_norm)\n",
    "centers = model.cluster_centers()\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# SAVE RESULTS\n",
    "# save hru names and their corresponding cluster labels\n",
    "# save training data so as to keep the same order \n",
    "# save \n",
    "# ---------------------------------------------\n",
    "zipped = zip(names_list, labels)\n",
    "clustering_custom = list(zipped)\n",
    "textfile = open(\"clustering_custom.txt\", \"w\")\n",
    "for element in clustering_custom:\n",
    "    print(element)\n",
    "    textfile.write(element[0]+' , ')\n",
    "    textfile.write(element[1].astype(str))\n",
    "    textfile.write('\\n')\n",
    "textfile.close()\n",
    "\n",
    "np.save(save_path+'norm_hru_clustering_data', X_train_norm)\n",
    "np.save(save_path+'hru_cluster_labels.npy', labels)\n",
    "np.save(save_path+'hru_cluster_distance', dist)\n",
    "np.save(save_path+'cluster_centers.npy', centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f5b3ae0745328dcc61aefa536325bd90100aa39ad9e0c20a14dd1c1a401e940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
