{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# IMPORT LIBRARIES\n",
    "# ---------------------------------------------\n",
    "import numpy as np\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax , TimeSeriesScalerMeanVariance\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# DEPENDENCIES \n",
    "# the following depend on 01_Gen_Clustering_Data:\n",
    "# - clustering_feature_names\n",
    "# - n_hrus\n",
    "# - all_data\n",
    "# ---------------------------------------------\n",
    "with open('/Users/Documents/GitHub/GNNs_PrecisionAgriculture/SWAT Data Reader/clustering_feature_names.pkl', 'rb') as f:\n",
    "    clustering_feature_names = pickle.load(f)\n",
    "n_feat = len(clustering_feature_names)\n",
    "\n",
    "data_path = '/Users/Documents/GitHub/DATA_SMest/HRU_clustering'\n",
    "names_list = os.listdir(data_path)\n",
    "names_list.remove('.DS_Store')\n",
    "\n",
    "# Since every watershed file has different number of HRUs \n",
    "# we strip the number of HRUs from the file name and \n",
    "# aggregate to allocate memory for all HRUs in region02\n",
    "n_hrus = np.array([x.split('.')[-2] for x in names_list]).astype(int)\n",
    "hrus_total = np.sum(n_hrus)\n",
    "all_data= np.zeros((12, hrus_total,10))   # (months, hrus, features)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# LOAD DATA\n",
    "# load all hrus from across different subbasins\n",
    "# to the same array 'all_data'\n",
    "# ---------------------------------------------\n",
    "st = 0\n",
    "en = 0\n",
    "for i, name in enumerate(names_list):\n",
    "    en += n_hrus[i]\n",
    "    all_data[:,st:en,:] = np.load(data_path+'/'+name)\n",
    "    st = en\n",
    "    \n",
    "# reshape data to (months, n_hrus, features)\n",
    "all_data.transpose(1,0,2)\n",
    "mean_t = np.mean(all_data,1)\n",
    "var_t = np.var(all_data,1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# SUBSET SAMPLING\n",
    "# Sample a subset of data that is 'close enough' \n",
    "# in mean and varaince sense to original data. \n",
    "# total hrus = 99047\n",
    "# sampled hrus = 1000\n",
    "# ---------------------------------------------\n",
    "min_var_diff = 100000\n",
    "min_mean_diff = 100000\n",
    "iterations = 1000\n",
    "for iter in range(iterations):\n",
    "    sample_ids = np.random.uniform(0,99047,1000).astype(int)\n",
    "    sample_data = all_data[:,sample_ids,:]\n",
    "    sample_mean_t = np.mean(sample_data,1)\n",
    "    sample_var_t = np.var(sample_data,1)\n",
    "    mean_diff = np.linalg.norm(mean_t - sample_mean_t)/np.linalg.norm(mean_t)\n",
    "    var_diff = np.linalg.norm(var_t - sample_var_t)/np.linalg.norm(var_t)\n",
    "    if (var_diff < min_var_diff) and (mean_diff < min_mean_diff):\n",
    "        min_var_diff = var_diff\n",
    "        min_mean_diff = mean_diff\n",
    "        min_sample_ids = sample_ids\n",
    "        min_sample_data = all_data[:,min_sample_ids,:]\n",
    "\n",
    "print('percent relative err mean: ', mean_diff*100)\n",
    "print('percent relative err var ' ,  var_diff*100)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# DEFINE NORMALIZATION AND CLUSTERING FUNCTIONS\n",
    "# ---------------------------------------------\n",
    "def normalization(type, X_train):\n",
    "    if type =='custom':\n",
    "        max_arr = np.zeros(9)\n",
    "        min_arr = np.zeros(9)\n",
    "        X_train_norm = np.zeros(X_train.shape)\n",
    "        for i in range(9):\n",
    "            max_arr[i] = np.max(X_train[:,:,i])\n",
    "            min_arr[i] = np.min(X_train[:,:,i])\n",
    "            X_train_norm[:,:,i] = (X_train[:,:,i] -  min_arr[i])/( max_arr[i]- min_arr[i])\n",
    "    elif type == 'minmax':\n",
    "        X_train_norm = TimeSeriesScalerMinMax(value_range=(0,1)).fit_transform(X_train)\n",
    "    elif type == 'std':\n",
    "        X_train_norm = TimeSeriesScalerMeanVariance(0,1).fit_transform(X_train)\n",
    "    else: \n",
    "        X_train_norm = X_train\n",
    "    return(X_train_norm)\n",
    "\n",
    "def elbow_iter (k,X_train_norm, n_sample):\n",
    "    model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=10)\n",
    "    model.fit(X_train_norm)\n",
    "    labels = model.labels_\n",
    "    dist = model.transform(X_train_norm)\n",
    "    dist_clust = np.zeros(n_sample)\n",
    "    for i in range (n_sample):\n",
    "        dist_clust[i] = dist[i,labels[i]]\n",
    "    distortion = np.average(dist_clust**2)\n",
    "    return(distortion)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# PERFORM ELBOW TEST\n",
    "# --------------------------------------------- \n",
    "# Normalize Data (CUSTOM)\n",
    "X_train_norm = normalization('custom', min_sample_data)\n",
    "\n",
    "# reshape data to (n_hrus, n_time samples, n_features) \n",
    "# for use with tslearn functions\n",
    "X_train_norm = X_train_norm.reshape(1000,12,10) \n",
    "\n",
    "# List of cluster numbers for elbow test\n",
    "k_list = [2,5,10,15,20,25,30, 40,50,80]\n",
    "distor = np.zeros(len(k_list))\n",
    "for i,k in enumerate(k_list):\n",
    "    distor[i] = elbow_iter(k,X_train_norm,1000)\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------\n",
    "# VISUALIZE RESULTS \n",
    "# --------------------------------------------- \n",
    "fs = 14\n",
    "plt.plot(np.array(k_list), distor)\n",
    "plt.xlabel('Number of Clusters (K)',fontsize=fs)\n",
    "plt.ylabel('Distortion', fontsize=fs)\n",
    "plt.title('Elbow Test for Selecting Number of Clusters (K)', fontsize=fs)\n",
    "plt.grid()\n",
    "plt.savefig('Elbow_Test_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f5b3ae0745328dcc61aefa536325bd90100aa39ad9e0c20a14dd1c1a401e940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
